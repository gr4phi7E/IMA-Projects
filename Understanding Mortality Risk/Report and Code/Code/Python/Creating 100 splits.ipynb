{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5343304-398c-4483-a4bc-f989be059e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[1s / 1s],\t\ttrain_loss: 3.6418,\tval_loss: 3.1728\n",
      "1:\t[1s / 2s],\t\ttrain_loss: 3.5117,\tval_loss: 3.2201\n",
      "2:\t[1s / 3s],\t\ttrain_loss: 3.5215,\tval_loss: 3.2181\n",
      "3:\t[1s / 4s],\t\ttrain_loss: 3.5306,\tval_loss: 3.2120\n",
      "4:\t[1s / 5s],\t\ttrain_loss: 3.5230,\tval_loss: 3.1649\n",
      "5:\t[1s / 6s],\t\ttrain_loss: 3.5299,\tval_loss: 3.1980\n",
      "6:\t[1s / 7s],\t\ttrain_loss: 3.5166,\tval_loss: 3.2092\n",
      "7:\t[1s / 8s],\t\ttrain_loss: 3.5074,\tval_loss: 3.2968\n",
      "8:\t[1s / 9s],\t\ttrain_loss: 3.4923,\tval_loss: 3.2343\n",
      "9:\t[1s / 10s],\t\ttrain_loss: 3.4947,\tval_loss: 3.2118\n",
      "10:\t[1s / 11s],\t\ttrain_loss: 3.5061,\tval_loss: 3.2786\n",
      "11:\t[1s / 12s],\t\ttrain_loss: 3.5194,\tval_loss: 3.2123\n",
      "12:\t[1s / 14s],\t\ttrain_loss: 3.5264,\tval_loss: 3.2045\n",
      "13:\t[1s / 15s],\t\ttrain_loss: 3.4778,\tval_loss: 3.2661\n",
      "14:\t[1s / 16s],\t\ttrain_loss: 3.5083,\tval_loss: 3.2898\n",
      "BiLSTM-CoxPH Concordance Index: 0.8022700532908852\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# 0. Import Libraries\n",
    "# ============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pycox.models import CoxPH\n",
    "from pycox.evaluation import EvalSurv\n",
    "import torchtuples as tt\n",
    "\n",
    "# ============================\n",
    "# 1. Configuration\n",
    "# ============================\n",
    "config = {\n",
    "    'lstm_hidden_size': 5,\n",
    "    'lstm_layers': 3,\n",
    "    'lstm_dropout': 0.3,\n",
    "    'batch_size': 64,\n",
    "    'epochs_lstm_cox': 512,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def to_device(x):\n",
    "    if isinstance(x, (tuple, list)):\n",
    "        return tuple(to_device(xx) for xx in x)\n",
    "    return x.to(device)\n",
    "\n",
    "# ============================\n",
    "# 2. Load and Prepare Dataset\n",
    "# ============================\n",
    "data = pd.read_csv(\"D:\\\\Final_Year_Project\\\\NHANES_11_14_survPA_Python.csv\")\n",
    "data_act = pd.read_csv(\"D:\\\\Final_Year_Project\\\\activity.csv\")\n",
    "df = pd.concat([data, data_act], axis=1)\n",
    "df['row_id'] = df.index\n",
    "\n",
    "# Shuffle and split\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_val = df.sample(frac=0.1, random_state=42).reset_index(drop=True)\n",
    "df_train = df.drop(df_val.index).reset_index(drop=True)\n",
    "\n",
    "# ============================\n",
    "# 3. Feature Columns\n",
    "# ============================\n",
    "static_cols = ['Age', 'Race', 'BMI', 'sex', 'Mobility', 'diabetes.y', 'poverty_level',\n",
    "               'Asthma', 'Arthritis', 'heart_failure', 'coronary_heart_disease',\n",
    "               'angina', 'stroke', 'thyroid', 'bronchitis', 'cancer']\n",
    "time_cols = [str(i) for i in range(1, 1441)]\n",
    "\n",
    "def prepare_data(df):\n",
    "    X_static = df[static_cols].copy()\n",
    "    X_time = df[time_cols].values.reshape(len(df), 1440, 1)\n",
    "    y_event = df['mortstat'].values.astype(np.float32)\n",
    "    y_time = df['time_mort'].values.astype(np.float32)\n",
    "    row_ids = df['row_id'].values\n",
    "    return X_static, X_time.astype(np.float32), y_time, y_event, row_ids\n",
    "\n",
    "X_train_static, X_train_time, time_train, y_train_event, row_ids_train = prepare_data(df_train)\n",
    "X_val_static, X_val_time, time_val, y_val_event, row_ids_val = prepare_data(df_val)\n",
    "\n",
    "# ============================\n",
    "# 4. BiLSTM Feature Extractor\n",
    "# ============================\n",
    "class BiLSTMFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=9, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            dropout=dropout,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        hn_forward = hn[-2]\n",
    "        hn_backward = hn[-1]\n",
    "        return torch.cat((hn_forward, hn_backward), dim=1)\n",
    "\n",
    "# ============================\n",
    "# 5. Wrapper for CoxPH Model\n",
    "# ============================\n",
    "class FeatureWrapperForCox(nn.Module):\n",
    "    def __init__(self, lstm_model, static_input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.lstm_model = lstm_model\n",
    "        self.linear = nn.Linear(2 * hidden_dim + static_input_dim, 1)\n",
    "\n",
    "    def forward(self, X_time, X_static):\n",
    "        lstm_feat = self.lstm_model(X_time)\n",
    "        return self.linear(torch.cat([lstm_feat, X_static], dim=1))\n",
    "\n",
    "# ============================\n",
    "# 6. Train BiLSTM with CoxPH Loss\n",
    "# ============================\n",
    "def train_bilstm_cox(X_train_time, X_train_static, durations_train, events_train,\n",
    "                     X_val_time, X_val_static, durations_val, events_val):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_static = scaler.fit_transform(X_train_static).astype(np.float32)\n",
    "    X_val_static = scaler.transform(X_val_static).astype(np.float32)\n",
    "\n",
    "    X_train_time, X_train_static, durations_train, events_train = to_device((\n",
    "        torch.tensor(X_train_time), torch.tensor(X_train_static),\n",
    "        torch.tensor(durations_train), torch.tensor(events_train)))\n",
    "    X_val_time, X_val_static, durations_val, events_val = to_device((\n",
    "        torch.tensor(X_val_time), torch.tensor(X_val_static),\n",
    "        torch.tensor(durations_val), torch.tensor(events_val)))\n",
    "\n",
    "    y_train = (durations_train, events_train)\n",
    "    y_val = (durations_val, events_val)\n",
    "\n",
    "    lstm_model = BiLSTMFeatureExtractor(\n",
    "        hidden_size=config['lstm_hidden_size'],\n",
    "        num_layers=config['lstm_layers'],\n",
    "        dropout=config['lstm_dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    feature_net = FeatureWrapperForCox(\n",
    "        lstm_model, X_train_static.shape[1], config['lstm_hidden_size']\n",
    "    ).to(device)\n",
    "\n",
    "    model = CoxPH(feature_net, tt.optim.Adam)\n",
    "    batch_size = config['batch_size']\n",
    "    model.optimizer.set_lr(model.lr_finder((X_train_time, X_train_static), y_train, batch_size=batch_size).get_best_lr())\n",
    "\n",
    "    model.fit((X_train_time, X_train_static), y_train, batch_size, epochs=config['epochs_lstm_cox'],\n",
    "              callbacks=[tt.callbacks.EarlyStopping()],\n",
    "              val_data=((X_val_time, X_val_static), y_val), val_batch_size=batch_size)\n",
    "\n",
    "    model.compute_baseline_hazards()\n",
    "    surv = model.predict_surv_df((X_val_time, X_val_static))\n",
    "    ev = EvalSurv(surv, durations_val.cpu().numpy(), events_val.cpu().numpy(), censor_surv='km')\n",
    "    print(\"BiLSTM-CoxPH Concordance Index:\", ev.concordance_td())\n",
    "\n",
    "    return lstm_model, scaler\n",
    "\n",
    "# ============================\n",
    "# 7. Extract Only Dynamic (LSTM) Features\n",
    "# ============================\n",
    "def extract_lstm_only_features(lstm_model, X_time):\n",
    "    X_time = torch.tensor(X_time).to(device)\n",
    "    with torch.no_grad():\n",
    "        lstm_feat = lstm_model(X_time)\n",
    "    return lstm_feat.cpu().numpy()\n",
    "\n",
    "# ============================\n",
    "# 8. Train and Extract Features\n",
    "# ============================\n",
    "lstm_model, scaler = train_bilstm_cox(X_train_time, X_train_static, time_train, y_train_event,\n",
    "                                      X_val_time, X_val_static, time_val, y_val_event)\n",
    "\n",
    "train_lstm_features = extract_lstm_only_features(lstm_model, X_train_time)\n",
    "val_lstm_features = extract_lstm_only_features(lstm_model, X_val_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc2cdf44-c369-41c7-ab41-49877a52db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 9. Save to DataFrames and Merge\n",
    "# ============================\n",
    "train_feat_df = pd.DataFrame(train_lstm_features, columns=[f\"dyn_feat_{i}\" for i in range(train_lstm_features.shape[1])])\n",
    "train_feat_df['row_id'] = row_ids_train\n",
    "\n",
    "val_feat_df = pd.DataFrame(val_lstm_features, columns=[f\"dyn_feat_{i}\" for i in range(val_lstm_features.shape[1])])\n",
    "val_feat_df['row_id'] = row_ids_val\n",
    "\n",
    "train_merged = pd.merge(df_train[static_cols + ['time_mort', 'mortstat', 'row_id']], train_feat_df, on='row_id')\n",
    "val_merged = pd.merge(df_val[static_cols + ['time_mort', 'mortstat', 'row_id']], val_feat_df, on='row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f4c3d86-3187-44d4-8ed9-a005ea91424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate row-wise\n",
    "df_LSTM = pd.concat([train_merged, val_merged], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d249d8f4-24a4-4022-b901-1fb18e491d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Race', 'BMI', 'sex', 'Mobility', 'diabetes.y', 'poverty_level',\n",
       "       'Asthma', 'Arthritis', 'heart_failure', 'coronary_heart_disease',\n",
       "       'angina', 'stroke', 'thyroid', 'bronchitis', 'cancer', 'time_mort',\n",
       "       'mortstat', 'row_id', 'dyn_feat_0', 'dyn_feat_1', 'dyn_feat_2',\n",
       "       'dyn_feat_3', 'dyn_feat_4', 'dyn_feat_5', 'dyn_feat_6', 'dyn_feat_7',\n",
       "       'dyn_feat_8', 'dyn_feat_9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_LSTM.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42d51264-adf2-4fa7-99cd-f94737b0e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = df_LSTM[['row_id', 'dyn_feat_0', 'dyn_feat_1', 'dyn_feat_2','dyn_feat_3', 'dyn_feat_4', 'dyn_feat_5', \n",
    "                  'dyn_feat_6', 'dyn_feat_7','dyn_feat_8', 'dyn_feat_9']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1945a9de-b4c2-42a7-945b-1a634c6d2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df, selected_df, on='row_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f769b34-03fe-4868-9877-27fe5c278641",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = merged_df[['Age', 'Race', 'BMI', 'sex', 'Mobility', 'diabetes.y', 'poverty_level','Asthma', 'Arthritis', \n",
    "                      'heart_failure', 'coronary_heart_disease','angina', 'stroke', 'thyroid', 'bronchitis', 'cancer', \n",
    "                      'time_mort','mortstat','PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','dyn_feat_0', 'dyn_feat_1', \n",
    "                      'dyn_feat_2','dyn_feat_3', 'dyn_feat_4', 'dyn_feat_5','dyn_feat_6', 'dyn_feat_7','dyn_feat_8', 'dyn_feat_9']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "325c872d-7e15-4891-a8ea-963be26155d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating split 1...\n",
      "Split 1 saved as train_split_1.csv and test_split_1.csv\n",
      "Generating split 2...\n",
      "Split 2 saved as train_split_2.csv and test_split_2.csv\n",
      "Generating split 3...\n",
      "Split 3 saved as train_split_3.csv and test_split_3.csv\n",
      "Generating split 4...\n",
      "Split 4 saved as train_split_4.csv and test_split_4.csv\n",
      "Generating split 5...\n",
      "Split 5 saved as train_split_5.csv and test_split_5.csv\n",
      "Generating split 6...\n",
      "Split 6 saved as train_split_6.csv and test_split_6.csv\n",
      "Generating split 7...\n",
      "Split 7 saved as train_split_7.csv and test_split_7.csv\n",
      "Generating split 8...\n",
      "Split 8 saved as train_split_8.csv and test_split_8.csv\n",
      "Generating split 9...\n",
      "Split 9 saved as train_split_9.csv and test_split_9.csv\n",
      "Generating split 10...\n",
      "Split 10 saved as train_split_10.csv and test_split_10.csv\n",
      "Generating split 11...\n",
      "Split 11 saved as train_split_11.csv and test_split_11.csv\n",
      "Generating split 12...\n",
      "Split 12 saved as train_split_12.csv and test_split_12.csv\n",
      "Generating split 13...\n",
      "Split 13 saved as train_split_13.csv and test_split_13.csv\n",
      "Generating split 14...\n",
      "Split 14 saved as train_split_14.csv and test_split_14.csv\n",
      "Generating split 15...\n",
      "Split 15 saved as train_split_15.csv and test_split_15.csv\n",
      "Generating split 16...\n",
      "Split 16 saved as train_split_16.csv and test_split_16.csv\n",
      "Generating split 17...\n",
      "Split 17 saved as train_split_17.csv and test_split_17.csv\n",
      "Generating split 18...\n",
      "Split 18 saved as train_split_18.csv and test_split_18.csv\n",
      "Generating split 19...\n",
      "Split 19 saved as train_split_19.csv and test_split_19.csv\n",
      "Generating split 20...\n",
      "Split 20 saved as train_split_20.csv and test_split_20.csv\n",
      "Generating split 21...\n",
      "Split 21 saved as train_split_21.csv and test_split_21.csv\n",
      "Generating split 22...\n",
      "Split 22 saved as train_split_22.csv and test_split_22.csv\n",
      "Generating split 23...\n",
      "Split 23 saved as train_split_23.csv and test_split_23.csv\n",
      "Generating split 24...\n",
      "Split 24 saved as train_split_24.csv and test_split_24.csv\n",
      "Generating split 25...\n",
      "Split 25 saved as train_split_25.csv and test_split_25.csv\n",
      "Generating split 26...\n",
      "Split 26 saved as train_split_26.csv and test_split_26.csv\n",
      "Generating split 27...\n",
      "Split 27 saved as train_split_27.csv and test_split_27.csv\n",
      "Generating split 28...\n",
      "Split 28 saved as train_split_28.csv and test_split_28.csv\n",
      "Generating split 29...\n",
      "Split 29 saved as train_split_29.csv and test_split_29.csv\n",
      "Generating split 30...\n",
      "Split 30 saved as train_split_30.csv and test_split_30.csv\n",
      "Generating split 31...\n",
      "Split 31 saved as train_split_31.csv and test_split_31.csv\n",
      "Generating split 32...\n",
      "Split 32 saved as train_split_32.csv and test_split_32.csv\n",
      "Generating split 33...\n",
      "Split 33 saved as train_split_33.csv and test_split_33.csv\n",
      "Generating split 34...\n",
      "Split 34 saved as train_split_34.csv and test_split_34.csv\n",
      "Generating split 35...\n",
      "Split 35 saved as train_split_35.csv and test_split_35.csv\n",
      "Generating split 36...\n",
      "Split 36 saved as train_split_36.csv and test_split_36.csv\n",
      "Generating split 37...\n",
      "Split 37 saved as train_split_37.csv and test_split_37.csv\n",
      "Generating split 38...\n",
      "Split 38 saved as train_split_38.csv and test_split_38.csv\n",
      "Generating split 39...\n",
      "Split 39 saved as train_split_39.csv and test_split_39.csv\n",
      "Generating split 40...\n",
      "Split 40 saved as train_split_40.csv and test_split_40.csv\n",
      "Generating split 41...\n",
      "Split 41 saved as train_split_41.csv and test_split_41.csv\n",
      "Generating split 42...\n",
      "Split 42 saved as train_split_42.csv and test_split_42.csv\n",
      "Generating split 43...\n",
      "Split 43 saved as train_split_43.csv and test_split_43.csv\n",
      "Generating split 44...\n",
      "Split 44 saved as train_split_44.csv and test_split_44.csv\n",
      "Generating split 45...\n",
      "Split 45 saved as train_split_45.csv and test_split_45.csv\n",
      "Generating split 46...\n",
      "Split 46 saved as train_split_46.csv and test_split_46.csv\n",
      "Generating split 47...\n",
      "Split 47 saved as train_split_47.csv and test_split_47.csv\n",
      "Generating split 48...\n",
      "Split 48 saved as train_split_48.csv and test_split_48.csv\n",
      "Generating split 49...\n",
      "Split 49 saved as train_split_49.csv and test_split_49.csv\n",
      "Generating split 50...\n",
      "Split 50 saved as train_split_50.csv and test_split_50.csv\n",
      "Generating split 51...\n",
      "Split 51 saved as train_split_51.csv and test_split_51.csv\n",
      "Generating split 52...\n",
      "Split 52 saved as train_split_52.csv and test_split_52.csv\n",
      "Generating split 53...\n",
      "Split 53 saved as train_split_53.csv and test_split_53.csv\n",
      "Generating split 54...\n",
      "Split 54 saved as train_split_54.csv and test_split_54.csv\n",
      "Generating split 55...\n",
      "Split 55 saved as train_split_55.csv and test_split_55.csv\n",
      "Generating split 56...\n",
      "Split 56 saved as train_split_56.csv and test_split_56.csv\n",
      "Generating split 57...\n",
      "Split 57 saved as train_split_57.csv and test_split_57.csv\n",
      "Generating split 58...\n",
      "Split 58 saved as train_split_58.csv and test_split_58.csv\n",
      "Generating split 59...\n",
      "Split 59 saved as train_split_59.csv and test_split_59.csv\n",
      "Generating split 60...\n",
      "Split 60 saved as train_split_60.csv and test_split_60.csv\n",
      "Generating split 61...\n",
      "Split 61 saved as train_split_61.csv and test_split_61.csv\n",
      "Generating split 62...\n",
      "Split 62 saved as train_split_62.csv and test_split_62.csv\n",
      "Generating split 63...\n",
      "Split 63 saved as train_split_63.csv and test_split_63.csv\n",
      "Generating split 64...\n",
      "Split 64 saved as train_split_64.csv and test_split_64.csv\n",
      "Generating split 65...\n",
      "Split 65 saved as train_split_65.csv and test_split_65.csv\n",
      "Generating split 66...\n",
      "Split 66 saved as train_split_66.csv and test_split_66.csv\n",
      "Generating split 67...\n",
      "Split 67 saved as train_split_67.csv and test_split_67.csv\n",
      "Generating split 68...\n",
      "Split 68 saved as train_split_68.csv and test_split_68.csv\n",
      "Generating split 69...\n",
      "Split 69 saved as train_split_69.csv and test_split_69.csv\n",
      "Generating split 70...\n",
      "Split 70 saved as train_split_70.csv and test_split_70.csv\n",
      "Generating split 71...\n",
      "Split 71 saved as train_split_71.csv and test_split_71.csv\n",
      "Generating split 72...\n",
      "Split 72 saved as train_split_72.csv and test_split_72.csv\n",
      "Generating split 73...\n",
      "Split 73 saved as train_split_73.csv and test_split_73.csv\n",
      "Generating split 74...\n",
      "Split 74 saved as train_split_74.csv and test_split_74.csv\n",
      "Generating split 75...\n",
      "Split 75 saved as train_split_75.csv and test_split_75.csv\n",
      "Generating split 76...\n",
      "Split 76 saved as train_split_76.csv and test_split_76.csv\n",
      "Generating split 77...\n",
      "Split 77 saved as train_split_77.csv and test_split_77.csv\n",
      "Generating split 78...\n",
      "Split 78 saved as train_split_78.csv and test_split_78.csv\n",
      "Generating split 79...\n",
      "Split 79 saved as train_split_79.csv and test_split_79.csv\n",
      "Generating split 80...\n",
      "Split 80 saved as train_split_80.csv and test_split_80.csv\n",
      "Generating split 81...\n",
      "Split 81 saved as train_split_81.csv and test_split_81.csv\n",
      "Generating split 82...\n",
      "Split 82 saved as train_split_82.csv and test_split_82.csv\n",
      "Generating split 83...\n",
      "Split 83 saved as train_split_83.csv and test_split_83.csv\n",
      "Generating split 84...\n",
      "Split 84 saved as train_split_84.csv and test_split_84.csv\n",
      "Generating split 85...\n",
      "Split 85 saved as train_split_85.csv and test_split_85.csv\n",
      "Generating split 86...\n",
      "Split 86 saved as train_split_86.csv and test_split_86.csv\n",
      "Generating split 87...\n",
      "Split 87 saved as train_split_87.csv and test_split_87.csv\n",
      "Generating split 88...\n",
      "Split 88 saved as train_split_88.csv and test_split_88.csv\n",
      "Generating split 89...\n",
      "Split 89 saved as train_split_89.csv and test_split_89.csv\n",
      "Generating split 90...\n",
      "Split 90 saved as train_split_90.csv and test_split_90.csv\n",
      "Generating split 91...\n",
      "Split 91 saved as train_split_91.csv and test_split_91.csv\n",
      "Generating split 92...\n",
      "Split 92 saved as train_split_92.csv and test_split_92.csv\n",
      "Generating split 93...\n",
      "Split 93 saved as train_split_93.csv and test_split_93.csv\n",
      "Generating split 94...\n",
      "Split 94 saved as train_split_94.csv and test_split_94.csv\n",
      "Generating split 95...\n",
      "Split 95 saved as train_split_95.csv and test_split_95.csv\n",
      "Generating split 96...\n",
      "Split 96 saved as train_split_96.csv and test_split_96.csv\n",
      "Generating split 97...\n",
      "Split 97 saved as train_split_97.csv and test_split_97.csv\n",
      "Generating split 98...\n",
      "Split 98 saved as train_split_98.csv and test_split_98.csv\n",
      "Generating split 99...\n",
      "Split 99 saved as train_split_99.csv and test_split_99.csv\n",
      "Generating split 100...\n",
      "Split 100 saved as train_split_100.csv and test_split_100.csv\n",
      "\n",
      "All splits generated and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure 'mortstat' exists and it's the event indicator column (True/False)\n",
    "if 'mortstat' not in df.columns:\n",
    "    raise ValueError(\"'mortstat' column not found in the data.\")\n",
    "\n",
    "# Define your target columns\n",
    "y = final_df['mortstat']  # Event indicator column\n",
    "X = final_df.drop(columns=['mortstat', 'time_mort'])  # Drop target and time column for features\n",
    "time_col = 'time_mort'  # Assuming 'time_mort' is your time column\n",
    "\n",
    "# Perform 100 stratified splits and save\n",
    "for i in range(1, 101):\n",
    "    print(f\"Generating split {i}...\")\n",
    "\n",
    "    # Perform stratified split (80/20)\n",
    "    train, test = train_test_split(final_df, test_size=0.2, stratify=y, random_state=i)\n",
    "\n",
    "    # Save to CSV\n",
    "    train_file = f\"train_split_{i}.csv\"\n",
    "    test_file = f\"test_split_{i}.csv\"\n",
    "\n",
    "    train.to_csv(train_file, index=False)\n",
    "    test.to_csv(test_file, index=False)\n",
    "\n",
    "    print(f\"Split {i} saved as {train_file} and {test_file}\")\n",
    "\n",
    "print(\"\\nAll splits generated and saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
